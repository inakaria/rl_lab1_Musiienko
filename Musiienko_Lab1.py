import numpy as np
import gymnasium as gym
import matplotlib.pyplot as plt
import seaborn as sns
import random


print("Milena Musiienko / KM-12 / Lab-1")

env = gym.make("FrozenLake-v1", is_slippery=True)
env = env.unwrapped

# –ü–∞—Ä–∞–º–µ—Ç—Ä–∏
gamma = 0.5
theta = 1e-6  # –ö—Ä–∏—Ç–µ—Ä—ñ–π –∑—É–ø–∏–Ω–∫–∏
n_states = env.observation_space.n
n_actions = env.action_space.n

policy = np.ones((n_states, n_actions)) / n_actions


print("\n2a. –û–±—á–∏—Å–ª–∏—Ç–∏ —Ñ—É–Ω–∫—Ü—ñ—é —Ü—ñ–Ω–∏ —Å—Ç–∞–Ω—É ùë£ùúã1(ùë†) –¥–ª—è —Ä—ñ–≤–Ω–æ–π–º–æ–≤—ñ—Ä–Ω–æ—ó (–≤–∏–ø–∞–¥–∫–æ–≤–æ—ó) —Å—Ç—Ä–∞—Ç–µ–≥—ñ—ó ùúã1", 
      "–ø—Ä–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä—ñ ùõæ = 0.5 –∑–∞ –¥–æ–ø–æ–º–æ–≥–æ—é —ñ—Ç–µ—Ä–∞—Ü—ñ–π–Ω–æ–≥–æ –∞–ª–≥–æ—Ä–∏—Ç–º—É –û—Ü—ñ–Ω—é–≤–∞–Ω–Ω—è —Å—Ç—Ä–∞—Ç–µ–≥—ñ—ó")

# –û—Ü—ñ–Ω—é–≤–∞–Ω–Ω—è —Å—Ç—Ä–∞—Ç–µ–≥—ñ—ó
def policy_evaluation(env, policy, gamma, theta):
    value_table = np.zeros(n_states)
    while True:
        delta = 0
        for s in range(n_states):
            v = 0
            for a in range(n_actions):
                for prob, next_state, reward, done in env.P[s][a]:
                    v += policy[s, a] * prob * (reward + gamma * value_table[next_state])
            delta = max(delta, abs(v - value_table[s]))
            value_table[s] = v
        if delta < theta:
            break
    return value_table

policy_evaluation_value = policy_evaluation(env, policy, gamma, theta)
policy_evaluation_value = policy_evaluation_value.reshape((4, 4))
print("Policy Evaluation:\n", policy_evaluation_value)


print("\n2b. –û–±—á–∏—Å–ª–∏—Ç–∏ —Ñ—É–Ω–∫—Ü—ñ—é —Ü—ñ–Ω–∏ —Å—Ç–∞–Ω—É ùë£ùúã1(ùë†) –¥–ª—è —Ä—ñ–≤–Ω–æ–π–º–æ–≤—ñ—Ä–Ω–æ—ó (–≤–∏–ø–∞–¥–∫–æ–≤–æ—ó) —Å—Ç—Ä–∞—Ç–µ–≥—ñ—ó ùúã1", 
      "–ø—Ä–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä—ñ ùõæ = 0.5 –∑–∞ –¥–æ–ø–æ–º–æ–≥–æ—é —Ä–æ–∑–≤‚Äô—è–∑–∞–Ω–Ω—è —Å–∏—Å—Ç–µ–º–∏ —Ä—ñ–≤–Ω—è–Ω—å –ë–µ–ª–º–∞–Ω–∞ –¥–ª—è —Ñ—É–Ω–∫—Ü—ñ—ó",
      "—Ü—ñ–Ω–∏ —Å—Ç–∞–Ω—É –≤—ñ–¥–Ω–æ—Å–Ω–æ –Ω–µ–≤—ñ–¥–æ–º–∏—Ö –∑–Ω–∞—á–µ–Ω—å ùë•ùëñ = ùë£ùúã1(ùë†ùëñ)")

# –†—ñ–≤–Ω—è–Ω–Ω—è –ë–µ–ª–º–∞–Ω–∞
def bellman(env, policy, gamma):
    A = np.zeros((n_states, n_states))
    b = np.zeros(n_states)
    for s in range(n_states):
        for a in range(n_actions):
            for prob, next_state, reward, done in env.P[s][a]:
                A[s, next_state] -= gamma * prob * policy[s, a]
                b[s] += prob * reward * policy[s, a]
        A[s, s] += 1
    v = np.linalg.solve(A, b)
    return v

bellman_value = bellman(env, policy, gamma)
bellman_value = bellman_value.reshape((4, 4))
print("Bellman:\n", bellman_value)


print("\n3. –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—é—á–∏ –∑–Ω–∞–π–¥–µ–Ω—ñ –∑–Ω–∞—á–µ–Ω–Ω—è —Ñ—É–Ω–∫—Ü—ñ—ó —Ü—ñ–Ω–∏ —Å—Ç–∞–Ω—É ùë£ùúã1(ùë†) —Ç–∞ —Ä—ñ–≤–Ω—è–Ω–Ω—è –ë–µ–ª–º–∞–Ω–∞",
      "–¥–ª—è —Ñ—É–Ω–∫—Ü—ñ—ó —Ü—ñ–Ω–∏ –¥—ñ—ó-—Å—Ç–∞–Ω—É, –æ—Ü—ñ–Ω–∏—Ç–∏ —Ñ—É–Ω–∫—Ü—ñ—é —Ü—ñ–Ω–∏ –¥—ñ—ó-—Å—Ç–∞–Ω—É ùëûùúã1(ùë†ùëñ,ùëéùëó)")

# –§—É–Ω–∫—Ü—ñ—è —Ü—ñ–Ω–∏ –¥—ñ—ó-—Å—Ç–∞–Ω—É
def action_value(env, value_table, gamma):
    action_value = np.zeros((n_states, n_actions))
    for s in range(n_states):
        for a in range(n_actions):
            q_sa = 0
            for prob, next_state, reward, done in env.P[s][a]:
                q_sa += prob * (reward + gamma * value_table[next_state])
            action_value[s, a] = q_sa
    return action_value

value_table_flat = policy_evaluation_value.flatten()
action_value = action_value(env, value_table_flat, gamma)
print("Action-Value Function:\n", action_value)


print("\n4. –°—Ç–≤–æ—Ä–∏—Ç–∏ —Ñ—É–Ω–∫—Ü—ñ—é equiprobable, —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–º —è–∫–æ—ó —î –Ω–æ–º–µ—Ä –¥—ñ—ó.",
      "–î—ñ—è –æ–±–∏—Ä–∞—î—Ç—å—Å—è –≤–∏–ø–∞–¥–∫–æ–≤–∏–º —á–∏–Ω–æ–º –∑ –º–Ω–æ–∂–∏–Ω–∏ –¥–æ–ø—É—Å—Ç–∏–º–∏—Ö –¥—ñ–π")

def equiprobable():
    actions = list(range(env.action_space.n))
    return random.choice(actions)

action = equiprobable()
print(f"Action number:", action)


print("\n5. –°—Ç–≤–æ—Ä–∏—Ç–∏  —Ñ—É–Ω–∫—Ü—ñ—é  get_episode,  —è–∫–∞  –ø—Ä–∏–π–º–∞—î  —É  —è–∫–æ—Å—Ç—ñ  –∞—Ä–≥—É–º–µ–Ω—Ç—É –µ–∫–∑–µ–º–ø–ª—è—Ä —Å–µ—Ä–µ–¥–æ–≤–∏—â–∞,", 
      "–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–º —Ñ—É–Ω–∫—Ü—ñ—ó —î –µ–ø—ñ–∑–æ–¥, —Ç–æ–±—Ç–æ —Å–ø–∏—Å–æ–∫ –∫–æ—Ä—Ç–µ–∂—ñ–≤,  –∫–æ–∂–µ–Ω  –∑  —è–∫–∏—Ö  –∑–±–µ—Ä—ñ–≥–∞—î  –≤—Å—ñ  —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∏",
      "–∫–æ–∂–Ω–æ–≥–æ  –∫—Ä–æ–∫—É –∞–≥–µ–Ω—Ç–∞  (—Ç–æ–±—Ç–æ  –ø–æ–ø–µ—Ä–µ–¥–Ω—ñ–π  —Å—Ç–∞–Ω,  –¥—ñ—é,  –≤–∏–Ω–∞–≥–æ—Ä–æ–¥—É,  –ø–æ—Ç–æ—á–Ω–∏–π  —Å—Ç–∞–Ω, –∑–Ω–∞—á–µ–Ω–Ω—è –ø–∞—Ä–∞–º–µ—Ç—Ä—ñ–≤ terminated —Ç–∞ truncated).",
      "–í–∏–±—ñ—Ä  –∞–≥–µ–Ω—Ç–æ–º  –¥—ñ—ó  —É  –∫–æ–∂–Ω–æ–º—É  —Å—Ç–∞–Ω—ñ  –Ω–∞  –¥–∞–Ω–æ–º—É  –µ—Ç–∞–ø—ñ  —Ä–µ–∞–ª—ñ–∑—É–π—Ç–µ  –Ω–∞ –æ—Å–Ω–æ–≤—ñ  —Ñ—É–Ω–∫—Ü—ñ—ó  equiprobable,",
      "–∞–±–æ,  —ñ–Ω—à–∏–º–∏  —Å–ª–æ–≤–∞–º–∏,  –Ω–∞  –æ—Å–Ω–æ–≤—ñ —Ä—ñ–≤–Ω–æ–π–º–æ–≤—ñ—Ä–Ω–æ—ó (–≤–∏–ø–∞–¥–∫–æ–≤–æ—ó) —Å—Ç—Ä–∞—Ç–µ–≥—ñ—ó ùúã1.")

# –§—É–Ω–∫—Ü—ñ—è –¥–ª—è —Å—Ç–≤–æ—Ä–µ–Ω–Ω—è –µ–ø—ñ–∑–æ–¥—É
def get_episode(env):
    episode = [] 
    state = env.reset()  # –Ü–Ω—ñ—Ü—ñ–∞–ª—ñ–∑—É–≤–∞—Ç–∏ —Å–µ—Ä–µ–¥–æ–≤–∏—â–µ —Ç–∞ –æ—Ç—Ä–∏–º–∞—Ç–∏ –ø–æ—á–∞—Ç–∫–æ–≤–∏–π —Å—Ç–∞–Ω
    while True:
        action = equiprobable()
        next_state, reward, terminated, truncated, info = env.step(action)
        episode.append((state, action, reward, next_state, terminated, truncated))
        if terminated or truncated:  # –Ø–∫—â–æ –µ–ø—ñ–∑–æ–¥ –∑–∞–≤–µ—Ä—à–µ–Ω–æ
            break
        state = next_state
    return episode

episode = get_episode(env)
print("\nEpisode:")
for step, (state, action, reward, next_state, terminated, truncated) in enumerate(episode):
    print(f"Step {step + 1}: State={state}, Action={action}, Reward={reward}, "
          f"Next state={next_state}, Terminated={terminated}, Truncated={truncated}")
    

print("\n6. –í–∏–∫–æ–Ω–∞—Ç–∏ 75 –µ–ø—ñ–∑–æ–¥—ñ–≤ –∑–∞ –¥–æ–ø–æ–º–æ–≥–æ—é —Ñ—É–Ω–∫—Ü—ñ—ó get_episode. –í–∏–≤–µ–¥—ñ—Ç—å –Ω–∞ –µ–∫—Ä–∞–Ω –¥–≤–∞ –≥—Ä–∞—Ñ—ñ–∫–∏: –≤–∏–Ω–∞–≥–æ—Ä–æ–¥–∞ —Ç–∞ —Ç—Ä–∏–≤–∞–ª—ñ—Å—Ç—å –µ–ø—ñ–∑–æ–¥—É.")

n_episodes = 75
total_rewards = []
total_durations = []

for i in range(n_episodes):
    episode = get_episode(env)
    reward = sum(step[2] for step in episode)
    total_rewards.append(reward)
    total_durations.append(len(episode))

fig, axes = plt.subplots(1, 2, figsize=(12, 6))

axes[0].plot(range(1, n_episodes + 1), total_rewards, color='r')
axes[0].set_title('Episode Rewards')
axes[0].set_xlabel('Episode')
axes[0].set_ylabel('Reward')

axes[1].plot(range(1, n_episodes + 1), total_durations, color='b')
axes[1].set_title('Episode Durations')
axes[1].set_xlabel('Episode')
axes[1].set_ylabel('Steps')

plt.savefig("task6.png", dpi=300, bbox_inches='tight')


print("\n7. –†–µ–∞–ª—ñ–∑—É–≤–∞—Ç–∏ –º–µ—Ç–æ–¥ –Ü—Ç–µ—Ä–∞—Ü—ñ—ó —Å—Ç—Ä–∞—Ç–µ–≥—ñ—ó (Policy Iteration) –¥–ª—è –∑–Ω–∞—Ö–æ–¥–∂–µ–Ω–Ω—è –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ—ó —Å—Ç—Ä–∞—Ç–µ–≥—ñ—ó –∑–∞ –∑–∞–¥–∞–Ω–æ—é –ø–æ—á–∞—Ç–∫–æ–≤–æ—é")

def policy_iteration(env, gamma, theta):
    n_states = env.observation_space.n
    n_actions = env.action_space.n
    value_table = np.zeros(n_states)

    # –û—Ü—ñ–Ω—é–≤–∞–Ω–Ω—è —Å—Ç—Ä–∞—Ç–µ–≥—ñ—ó
    while True:
        delta = 0
        for s in range(n_states):
            q_values = []
            for a in range(n_actions):
                q_sa = 0
                for prob, next_state, reward, done in env.P[s][a]:
                    q_sa += prob * (reward + gamma * value_table[next_state])
                q_values.append(q_sa)
            max_value = max(q_values)
            delta = max(delta, abs(max_value - value_table[s]))
            value_table[s] = max_value
        if delta < theta:
            break

    # –ü–æ–∫—Ä–∞—â–µ–Ω–Ω—è —Å—Ç—Ä–∞—Ç–µ–≥—ñ—ó
    policy = np.zeros((n_states, n_actions))
    for s in range(n_states):
        q_values = []
        for a in range(n_actions):
            q_sa = 0
            for prob, next_state, reward, done in env.P[s][a]:
                q_sa += prob * (reward + gamma * value_table[next_state])
            q_values.append(q_sa)
        best_action = np.argmax(q_values)
        policy[s, best_action] = 1.0

    return value_table, policy


optimal_value, optimal_policy = policy_iteration(env, gamma, theta)
optimal_value = optimal_value.reshape((4, 4))

print("Optimal Policy:")
print(optimal_policy)
print("\nOptimal Value Function:")
print(optimal_value)

plt.figure(figsize=(6, 6))
sns.heatmap(optimal_value, annot=True, fmt=".2f")
plt.title("Optimal Value Function")
plt.savefig("task7.png", dpi=300, bbox_inches='tight')


print("\n8. –û—Ü—ñ–Ω–∏—Ç–∏  –æ–ø—Ç–∏–º–∞–ª—å–Ω—É  —Å—Ç—Ä–∞—Ç–µ–≥—ñ—é  ùúã‚àó  —Ç–∞  —Ñ—É–Ω–∫—Ü—ñ—é  —Ü—ñ–Ω–∏  —Å—Ç–∞–Ω—É  ùë£‚àó(ùë†)  –¥–ª—è –∑–∞–¥–∞–Ω–æ–≥–æ  —Å–µ—Ä–µ–¥–æ–≤–∏—â–∞  –∑–∞",
      "–¥–æ–ø–æ–º–æ–≥–æ—é  –º–µ—Ç–æ–¥—É  –Ü—Ç–µ—Ä–∞—Ü—ñ—ó  —Å—Ç—Ä–∞—Ç–µ–≥—ñ—ó, –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—é—á–∏  –≤  —è–∫–æ—Å—Ç—ñ  –ø–æ—á–∞—Ç–∫–æ–≤–∏—Ö  –ø–∞—Ä–∞–º–µ—Ç—Ä—ñ–≤  —Å—Ç—Ä–∞—Ç–µ–≥—ñ—é  ùúã1  —Ç–∞",
      "—Ñ—É–Ω–∫—Ü—ñ—é  —Ü—ñ–Ω–∏  ùë£ùúã1(ùë†).  –ü–æ—Ä—ñ–≤–Ω—è–π—Ç–µ  –æ—Ç—Ä–∏–º–∞–Ω—É  —Ñ—É–Ω–∫—Ü—ñ—é  —Ü—ñ–Ω–∏  ùë£‚àó(ùë†)  –∑ —Ñ—É–Ω–∫—Ü—ñ—î—é ùë£ùúã1(ùë†) –∑ –∑–∞–≤–¥–∞–Ω–Ω—è N2.")

difference = optimal_value - policy_evaluation_value

fig, axes = plt.subplots(1, 3, figsize=(18, 6))
sns.heatmap(optimal_value, annot=True, fmt=".2f", ax=axes[0])
axes[0].set_title("Optimal v*(s)")
sns.heatmap(policy_evaluation_value, annot=True, fmt=".2f", ax=axes[1])
axes[1].set_title("Iterative v_{œÄ1}(s)")
sns.heatmap(difference, annot=True, fmt=".2f", ax=axes[2])
axes[2].set_title("Difference (v*(s) - v_{œÄ1}(s))")

plt.savefig("task8.png", dpi=300, bbox_inches='tight')


print("\n9. –í–∏–∫–æ–Ω–∞—Ç–∏ 12 –µ–ø—ñ–∑–æ–¥—ñ–≤ –∑–∞ –¥–æ–ø–æ–º–æ–≥–æ—é —Ñ—É–Ω–∫—Ü—ñ—ó get_episode –∑—ñ —Å—Ç—Ä–∞—Ç–µ–≥—ñ—î—é ùúã‚àó.",
      "–í–∏–≤–µ–¥—ñ—Ç—å –Ω–∞ –µ–∫—Ä–∞–Ω –¥–≤–∞ –≥—Ä–∞—Ñ—ñ–∫–∏: –≤–∏–Ω–∞–≥–æ—Ä–æ–¥–∞ —Ç–∞ —Ç—Ä–∏–≤–∞–ª—ñ—Å—Ç—å –µ–ø—ñ–∑–æ–¥—É. –ü–æ—Ä—ñ–≤–Ω—è–π—Ç–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∏ –∑ –≤—ñ–¥–ø–æ–≤—ñ–¥–Ω–∏–º–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏ –∑–∞–≤–¥–∞–Ω–Ω—è 6.")

n_episodes = 12
total_rewards_optimal = []
total_durations_optimal = []

def optimal_episodes(env, policy):
    episode = [] 
    state = env.reset()[0]  # –Ü–Ω—ñ—Ü—ñ–∞–ª—ñ–∑—É–≤–∞—Ç–∏ —Å–µ—Ä–µ–¥–æ–≤–∏—â–µ —Ç–∞ –æ—Ç—Ä–∏–º–∞—Ç–∏ –ø–æ—á–∞—Ç–∫–æ–≤–∏–π —Å—Ç–∞–Ω
    while True:
        action = np.argmax(policy[state])  # –í–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω–Ω—è –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ—ó —Å—Ç—Ä–∞—Ç–µ–≥—ñ—ó
        next_state, reward, terminated, truncated, info = env.step(action)
        episode.append((state, action, reward, next_state, terminated, truncated))
        if terminated or truncated:
            break
        state = next_state
    return episode

for i in range(n_episodes):
    episode = optimal_episodes(env, optimal_policy)
    total_rewards_optimal.append(sum(step[2] for step in episode))
    total_durations_optimal.append(len(episode))


fig, axes = plt.subplots(1, 2, figsize=(12, 6))

axes[0].plot(range(1, n_episodes + 1), total_rewards_optimal, color='g', label='Optimal Policy')
axes[0].set_title('Episode Rewards (Optimal Policy)')
axes[0].set_xlabel('Episode')
axes[0].set_ylabel('Reward')

axes[1].plot(range(1, n_episodes + 1), total_durations_optimal, color='c', label='Optimal Policy')
axes[1].set_title('Episode Durations (Optimal Policy)')
axes[1].set_xlabel('Episode')
axes[1].set_ylabel('Steps')

plt.savefig("task9.png", dpi=300, bbox_inches='tight')


print("\n10. –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—é—á–∏ –∑–Ω–∞–π–¥–µ–Ω—ñ –∑–Ω–∞—á–µ–Ω–Ω—è –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ—ó —Ñ—É–Ω–∫—Ü—ñ—ó —Ü—ñ–Ω–∏ —Å—Ç–∞–Ω—É ùë£‚àó(ùë†) —Ç–∞  —Ä—ñ–≤–Ω—è–Ω–Ω—è  –ë–µ–ª–º–∞–Ω–∞",
      "–¥–ª—è  —Ñ—É–Ω–∫—Ü—ñ—ó  —Ü—ñ–Ω–∏  –¥—ñ—ó-—Å—Ç–∞–Ω—É,  –æ—Ü—ñ–Ω–∏—Ç–∏ –æ–ø—Ç–∏–º–∞–ª—å–Ω—É —Ñ—É–Ω–∫—Ü—ñ—é —Ü—ñ–Ω–∏ –¥—ñ—ó-—Å—Ç–∞–Ω—É ùëû‚àó(ùë†ùëñ,ùëéùëó). –ü–æ—Ä—ñ–≤–Ω—è–π—Ç–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∏ –∑ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏ –∑–∞–≤–¥–∞–Ω–Ω—è N3")

def optimal_action_value(env, optimal_value_table, gamma):
    """–û–±—á–∏—Å–ª–µ–Ω–Ω—è –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ—ó —Ñ—É–Ω–∫—Ü—ñ—ó —Ü—ñ–Ω–∏ –¥—ñ—ó-—Å—Ç–∞–Ω—É q*(s, a)."""
    optimal_action = np.zeros((n_states, n_actions))
    for s in range(n_states):
        for a in range(n_actions):
            q_sa = 0
            for prob, next_state, reward, done in env.P[s][a]:
                q_sa += prob * (reward + gamma * optimal_value_table[next_state])
            optimal_action[s, a] = q_sa
    return optimal_action


optimal_action = optimal_action_value(env, optimal_value.flatten(), gamma)
difference_q = optimal_action - action_value

fig, axes = plt.subplots(1, 3, figsize=(18, 6))
sns.heatmap(optimal_action, annot=True, fmt=".2f", ax=axes[0])
axes[0].set_title("Optimal Action-Value Function q*")
sns.heatmap(action_value, annot=True, fmt=".2f", ax=axes[1])
axes[1].set_title("Action-Value Function q_œÄ1")
sns.heatmap(difference_q, annot=True, fmt=".2f", ax=axes[2])
axes[2].set_title("Difference (q* - q_œÄ1)")

plt.savefig("task10.png", dpi=300, bbox_inches='tight')


print("\n11. –°—Ç–≤–æ—Ä–∏—Ç–∏  —Ñ—É–Ω–∫—Ü—ñ—é  eps_greedy_policy,  –∞—Ä–≥—É–º–µ–Ω—Ç–∞–º–∏  —è–∫–æ—ó  —î  –º–∞—Å–∏–≤ –∑–Ω–∞—á–µ–Ω—å —Ñ—É–Ω–∫—Ü—ñ—ó",
      "—Ü—ñ–Ω–∏ –¥—ñ—ó-—Å—Ç–∞–Ω—É ùëû(ùë†,ùëé) —Ç–∞ –ø–∞—Ä–∞–º–µ—Ç—Ä ùúÄ, —Ç–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–º —î –Ω–æ–º–µ—Ä  –¥—ñ—ó,  –æ–±—Ä–∞–Ω–∏–π  –∑  –º–Ω–æ–∂–∏–Ω–∏  –Ω–æ–º–µ—Ä—ñ–≤",
      "–¥–æ–ø—É—Å—Ç–∏–º–∏—Ö  –¥—ñ–π  –¥–æ–ø–æ–º–æ–≥–æ—é –º–µ—Ç–æ–¥—É ùúÄ-–∂–∞–¥—ñ–±–Ω–æ—ó —Å—Ç—Ä–∞—Ç–µ–≥—ñ—ó (ùúÄ-greedy policy).")

def eps_greedy_policy(q_values, epsilon):
    n_actions = len(q_values)
    if np.random.rand() < epsilon:
        return np.random.choice(n_actions)
    else:
        return np.argmax(q_values)

q_values_example = [0.4, 0.2, 0.5, 0.9]
epsilon_example = 0.1
action = eps_greedy_policy(q_values_example, epsilon_example)
print(f"Action: {action}")
